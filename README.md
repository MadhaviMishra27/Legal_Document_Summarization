# âš–ï¸ Legal Document Summarizer â€” Indian Legal Judgments

## ğŸ”¹ Overview

This project focuses on **simplifying and summarizing Indian legal documents** (such as court judgments and petitions) to make them more accessible for the general public.  
The system follows the **exact pipeline described in the referenced research paper** â€” a **hybrid transformer-based summarization approach** that combines extractive and abstractive methods.

**Input â†’** A long Indian legal document (PDF or TXT)  
**Output â†’** A short, fluent, and simplified summary in plain language.

---

## ğŸ§  Research Basis

This work is implemented by following every step of the research paper:

> **â€œA Hybrid Transformer-Based Framework for Multi-Document Summarization of Turkish Legal Documentsâ€**

All core components (scraping, preprocessing, extractive, abstractive, hybrid, and evaluation) replicate the methodology proposed in the paper â€” only adapted to Indian legal text.

---

## ğŸ—ï¸ Project Workflow

1. **Data Scraping:** Extract Indian judgments from *Indian Kanoon* using BeautifulSoup.  
2. **Preprocessing:** Clean HTML â†’ text, remove noise, tokenize, and mark important outcomes.  
3. **Extractive Summarization:** Use **TF-IDF** and **TextRank** to pick key sentences.  
4. **Abstractive Summarization:** Use **MT5-small** and **DistilBART (bart-small)** for fluent summaries.  
5. **Hybrid Summarization:** Combine extractive + abstractive results for better coherence.  
6. **Evaluation:** Compute **ROUGE-1**, **ROUGE-2**, **ROUGE-L**, cosine, precision, recall, and F1 scores.  
7. **Model Saving:** Save best model (by ROUGE-Sum) to `/models/final`.  
8. **Prediction:** Summarize any unseen PDF/TXT legal document.  
9. **Deployment:** Streamlit web app for real-time summarization.

---

## ğŸ“‚ Folder Structure

CAPSTONE/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Raw scraped HTML files
â”‚ â”œâ”€â”€ preprocessed/ # Cleaned text ready for summarization
â”‚ â”œâ”€â”€ pre_processed/ # Used in evaluation
â”‚ â”œâ”€â”€ legal_stopwords.txt # Domain-specific stopwords
â”‚
â”œâ”€â”€ images/ # Streamlit dashboard screenshots
â”‚ â”œâ”€â”€ mainframe.png
â”‚ â”œâ”€â”€ image1.png
â”‚ â””â”€â”€ image2.png
â”‚
â”œâ”€â”€ models/
â”‚ â””â”€â”€ final/ # Best-performing model (bart-small)
â”‚ â”œâ”€â”€ config.json
â”‚ â”œâ”€â”€ generation_config.json
â”‚ â”œâ”€â”€ merges.txt
â”‚ â”œâ”€â”€ model.safetensors
â”‚ â”œâ”€â”€ tokenizer_config.json
â”‚ â”œâ”€â”€ tokenizer.json
â”‚ â””â”€â”€ vocab.json
â”‚
â”œâ”€â”€ reports/
â”‚ â”œâ”€â”€ metrics/ # CSVs (extractive, abstractive, model comparison)
â”‚ â””â”€â”€ visuals/ # ROUGE/F1 comparison plots
â”‚
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ extractive/ # Extractive summaries (TF-IDF, TextRank)
â”‚ â”œâ”€â”€ abstractive/ # Abstractive summaries (MT5, BART)
â”‚ â””â”€â”€ hybrid/ # Combined hybrid summaries
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ scrape_data.py # Web scraping (Indian Kanoon)
â”‚ â”œâ”€â”€ pre_processing.py # Cleaning & sentence tokenization
â”‚ â”œâ”€â”€ extractive.py # TF-IDF / TextRank summarization
â”‚ â”œâ”€â”€ abstractive.py # Transformer summarization (MT5 / BART)
â”‚ â”œâ”€â”€ hybrid_summary.py # Combine extractive + abstractive
â”‚ â”œâ”€â”€ evaluate.py # ROUGE/F1 metrics & plots
â”‚ â”œâ”€â”€ save_model.py # Save best model based on ROUGE-Sum
â”‚ â”œâ”€â”€ prediction.py # Summarize new PDF/TXT files
â”‚ â””â”€â”€ app.py # Streamlit web dashboard
â”‚
â”œâ”€â”€ reports/
â”‚
â”œâ”€â”€ Test_Files/ # Input PDFs/TXT for testing
â”œâ”€â”€ Test_Output/ # Generated summaries for test inputs
â”‚
â”œâ”€â”€ venv/ # Virtual environment
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ code.ipynb # Notebook version of workflow
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## âš™ï¸ Features

âœ… Web scraping of Indian legal judgments  
âœ… Data preprocessing with HTML cleaning, tokenization, and filtering  
âœ… Extractive summarization using TF-IDF and TextRank  
âœ… Abstractive summarization using **google/mt5-small** and **sshleifer/distilbart-cnn-12-6**  
âœ… Hybrid summarization integrating both methods  
âœ… Evaluation metrics (ROUGE, cosine, precision, recall, F1)  
âœ… Streamlit app for interactive summarization of PDF/TXT documents  

---

## ğŸ§° Installation

git clone https://github.com/your-username/Capstone_Project.git
cd Capstone_Project

## Sample Output

Extractive aggregated metrics (paper-style):
     method  cosine_mean  precision_mean  recall_mean  f1_mean
0     tfidf          1.0            1.0          1.0      1.0
1  textrank          1.0            1.0          1.0      1.0

Abstractive aggregated metrics (paper-style):
        model  rouge-1_mean  rouge-2_mean  rouge-L_mean  rouge-sum_mean
0  bart-small      0.326053      0.293145      0.304083        0.307760
1         mt5      0.022798      0.010405      0.022240        0.018481

## Run Streamlit Dashboard
streamlit run src/app.py

## ğŸ–¼ï¸ Streamlit App Preview

ğŸ“œ Another Example Output

The app allows users to upload .pdf or .txt files, automatically extract the text, and generate a simplified summary using the best-performing model (bart-small, fine-tuned on legal data).

##ğŸ”¹ Streamlit App Preview

Below are screenshots of the deployed legal summarizer dashboard built using Streamlit:

### ğŸ  Main Dashboard
![Streamlit Dashboard](images/dashboard.png)

### ğŸ“„ Example PDF Summary Output
![PDF Summary Output](images/sample_image1.png)
![PDF Summary Output](images/sample_image2.png)

The interface allows users to upload `.pdf` or `.txt` files, automatically extract text, and generate a simplified summary using the best-performing abstractive model (`bart-small` fine-tuned on legal text).


## ğŸ“Š Evaluation Metrics
Metric	Description
ROUGE-1 / 2 / L	Text overlap between generated and reference summaries
Cosine, Precision, Recall, F1	Extractive evaluation metrics
Word Reduction %	Text compression measure

## ğŸ”® Future Work

Add multilingual summarization (Hindi, Tamil, etc.)

Integrate Retrieval-Augmented Generation (RAG)

Deploy on Streamlit Cloud or Hugging Face Spaces

Enable live court case query and automatic summary generation

## âš–ï¸ License

This project is for academic and research purposes only.
All datasets and models follow their respective open licenses.
